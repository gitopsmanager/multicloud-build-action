# action.yml
name: Multicloud Build (composite)
description: >
  Build and push Docker images to AWS ECR or Azure ACR using Docker/BuildKit.
  Supports AKS Workload Identity, Azure MSI/client secret, and EKS Pod Identity or static keys.
  Loads default registries from a CD repo, optional BuildKit caching, and ECR repo auto-create.

author: Affinity7 Consulting Ltd
branding:
  icon: package
  color: blue

inputs:
  image_details:
    description: |
      JSON array of {context, build_file, image_name} or specify a single build with separate parmeters. 
      Example:
      [
        {"context":"./services/a","build_file":"Dockerfile","image_name":"team/svc-a"},
        {"context":"./services/b","build_file":"Dockerfile","image_name":"team/svc-b"}
      ]
    required: false
    type: string
  path:
    description: "Path to the build context"
    required: false
    default: "."
  image:
    description: "Image name (e.g. gitopsmanager)"
    required: true
  tag:
    description: "Tag for the image"
    required: false
    default: ""
  build_file:
    description: "Full path to the Dockerfile (relative to context)"
    required: false
    default: "Dockerfile"
  extra_args:
    description: "Additional buildkit args"
    required: false
    default: ""

  cd_repo:
    description: "CD repo (owner/repo) with cd/config/env-map.yaml providing default registries"
    required: true

  push:
    description: "Where to push the image (aws | azure | both | none)"
    required: false
    default: "none"

  buildkit_cache_mode:
    description: "BuildKit cache mode (min|max|none). Caches only to the current cloud."
    required: false
    default: "max"

  # GitHub App for CD repo checkout
  cd_app_id:
    description: "GitHub App ID for CD repo access"
    required: true
  cd_app_private_key:
    description: "GitHub App private key (PEM) for CD repo access"
    required: true

  # Azure credentials (used if not on AKS WI/MSI)
  azure_client_id:
    description: "Azure App/Identity client ID"
    required: false
    default: ""
  azure_client_secret:
    description: "Azure client secret"
    required: false
    default: ""
  azure_tenant_id:
    description: "Azure tenant ID"
    required: false
    default: ""

  # AWS credentials (used if not on EKS Pod Identity)
  aws_access_key_id:
    description: "AWS access key ID"
    required: false
    default: ""
  aws_secret_access_key:
    description: "AWS secret access key"
    required: false
    default: ""

runs:
  using: composite
  steps:
    - name: 🔄 Checkout code
      uses: actions/checkout@v4

    - name: 🔑 Generate GitHub App token
      id: generate_token
      uses: tibdex/github-app-token@v2.1.0
      with:
        app_id: ${{ inputs.cd_app_id }}
        private_key: ${{ inputs.cd_app_private_key }}
        installation_retrieval_mode: repository
        installation_retrieval_payload: ${{ inputs.cd_repo }}
        revoke: true



    - name: 📂 Checkout CD repo
      uses: actions/checkout@v4
      with:
        repository: ${{ inputs.cd_repo }}
        token: ${{ steps.generate_token.outputs.token }}
        path: cd

    - name: 📑 Load environment config
      id: env
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const cfg = JSON.parse(fs.readFileSync('cd/config/env-map.json', 'utf8'));
          core.setOutput('aws_registry',   cfg.build.aws_default_container_registry);
          core.setOutput('azure_registry', cfg.build.azure_default_container_registry);



    - name: Ensure Azure CLI (Ubuntu)
      if: inputs.push == 'azure' || inputs.push == 'both'
      shell: bash
      run: |
        set -euo pipefail

        if command -v az >/dev/null 2>&1; then
          echo "✅ Azure CLI found"
          az version
          exit 0
        fi

        if command -v apt-get >/dev/null 2>&1; then
          SUDO=""
          if [ "$(id -u)" -ne 0 ]; then SUDO="sudo"; fi
          echo "ℹ️ Installing Azure CLI via apt (Ubuntu)..."
          curl -sL https://aka.ms/InstallAzureCLIDeb | $SUDO bash
          echo "✅ Azure CLI installed"
          az version
        else
          echo "❌ apt-get not found — this step is Ubuntu-only." >&2
          exit 1
        fi



    # 🔐 Azure login (auto: WI → MSI → secret). No GitHub OIDC needed.
    - name: 🔐 Azure login (auto)
      if: inputs.push == 'azure' || inputs.push == 'both'
      shell: bash
      run: |
        set -euo pipefail
        login_done=0

        # 1) AKS Workload Identity (token file)
        if [ -n "${AZURE_FEDERATED_TOKEN_FILE:-}" ] && [ -n "${AZURE_CLIENT_ID:-}" ] && [ -n "${AZURE_TENANT_ID:-}" ]; then
          echo "Using AKS Workload Identity"
          az login --service-principal \
            --username "$AZURE_CLIENT_ID" \
            --tenant   "$AZURE_TENANT_ID" \
            --federated-token "$(cat "$AZURE_FEDERATED_TOKEN_FILE")" \
            --allow-no-subscriptions
          login_done=1
        fi

        # 2) Node MSI (Azure VM/VMSS)
        if [ "$login_done" -eq 0 ] && curl -fsS -m 1 -H "Metadata: true" \
             "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01" >/dev/null 2>&1; then
          echo "Using node Managed Identity (MSI)"
          az login --identity ${AZURE_CLIENT_ID:+--username "$AZURE_CLIENT_ID"} --allow-no-subscriptions
          login_done=1
        fi

        # 3) Client secret fallback (use workflow inputs directly; no env vars)
        if [ "$login_done" -eq 0 ] && \
          [ -n '${{ inputs.azure_client_id }}' ] && \
          [ -n '${{ inputs.azure_client_secret }}' ] && \
          [ -n '${{ inputs.azure_tenant_id }}' ]; then
          echo "Using Azure client secret (inputs)"
          az login --service-principal \
            --username '${{ inputs.azure_client_id }}' \
            --password '${{ inputs.azure_client_secret }}' \
            --tenant   '${{ inputs.azure_tenant_id }}' \
            --allow-no-subscriptions
          login_done=1
        fi


        if [ "$login_done" -eq 0 ]; then
          echo "❌ No Azure auth method available (WI/MSI/secret). Set AZURE_CLIENT_ID[/SECRET]/AZURE_TENANT_ID or run on AKS/VM with identity." >&2
          exit 1
        fi

    - name: 🔐 ACR Login (if pushing to Azure)
      if: inputs.push == 'azure' || inputs.push == 'both'
      shell: bash
      run: |
        set -euo pipefail
        TOKEN=$(az acr login --name "${{ steps.env.outputs.azure_registry }}" --expose-token -o tsv --query accessToken)
        echo $TOKEN | docker login "${{ steps.env.outputs.azure_registry }}" --username 00000000–0000–0000–0000–000000000000 --password-stdin

    - name: 📦 Install AWS CLI (if pushing to AWS)
      if: inputs.push == 'aws' || inputs.push == 'both'
      shell: bash
      run: |
        set -e
        if ! command -v aws >/dev/null 2>&1; then
          curl -sL "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o awscliv2.zip
          unzip -q awscliv2.zip
          sudo ./aws/install
        fi

    - name: ✅ Ensure ECR repository exists
      if: inputs.push == 'aws' || inputs.push == 'both'
      shell: bash
      env:
        AWS_ACCESS_KEY_ID: ${{ inputs.aws_access_key_id }}
        AWS_SECRET_ACCESS_KEY: ${{ inputs.aws_secret_access_key }}
      run: |
        set -euo pipefail
        REG="${{ steps.env.outputs.aws_registry }}"            # e.g. 123456789012.dkr.ecr.eu-west-1.amazonaws.com
        REPO="${{ inputs.image }}"                             # e.g. myteam/myapp
        REGION="$(echo "$REG" | cut -d. -f4)"                  # -> eu-west-1
        export AWS_DEFAULT_REGION="$REGION"
        if ! aws ecr describe-repositories --repository-names "$REPO" --region "$REGION" >/dev/null 2>&1; then
          echo "Creating ECR repository '$REPO' in $REGION"
          aws ecr create-repository \
            --repository-name "$REPO" \
            --image-scanning-configuration scanOnPush=true \
            --region "$REGION" >/dev/null || echo "Repo already exists, continuing."
        else
          echo "ECR repository '$REPO' already exists."
        fi

    - name: 🔐 ECR login
      if: inputs.push == 'aws' || inputs.push == 'both'
      shell: bash
      env:
        AWS_ACCESS_KEY_ID: ${{ inputs.aws_access_key_id }}
        AWS_SECRET_ACCESS_KEY: ${{ inputs.aws_secret_access_key }}
      run: |
        set -euo pipefail
        REG="${{ steps.env.outputs.aws_registry }}"
        REGION="$(echo "$REG" | cut -d. -f4)"   # parse region from registry
        export AWS_DEFAULT_REGION="$REGION"
        aws ecr get-login-password --region "$REGION" \
          | docker login --username AWS --password-stdin "$REG"


    - name: normalize-inputs
      id: normalize
      uses: actions/github-script@v7
      env:
        IMAGE_DETAILS: ${{ inputs.image_details }}
        SINGLE_IMAGE:  ${{ inputs.image }}
        SINGLE_DF:     ${{ inputs.build_file }}
        SINGLE_CTX:    ${{ inputs.path }}
      with:
        script: |
          const raw   = (process.env.IMAGE_DETAILS || '').trim();
          const image = (process.env.SINGLE_IMAGE  || '').trim();
          const df    = (process.env.SINGLE_DF     || '').trim();
          const ctx   = (process.env.SINGLE_CTX    || '.').trim() || '.';

          let items;
          if (raw) {
            items = JSON.parse(raw);
            if (!Array.isArray(items)) throw new Error('image_details must be a JSON array');
          } else if (image && df) {
            items = [{ context: ctx, build_file: df, image_name: image }];
          } else {
            throw new Error('Provide either inputs.image_details (JSON array) OR both inputs.image and inputs.build_file (and optional inputs.path).');
          }

          items.forEach((it, i) => {
            const miss = ['context','build_file','image_name'].filter(k => !it?.[k]);
            if (miss.length) throw new Error(`items[${i}] missing: ${miss.join(', ')}`);
          });

          core.setOutput('items', JSON.stringify(items));
          core.setOutput('count', String(items.length));

          // 🔎 Print normalized inputs for debugging
          core.startGroup('normalized items');
          core.info(JSON.stringify(items, null, 2));
          core.info(`count: ${items.length}`);
          core.endGroup();

   
    - name: 🐳 Set up Docker Buildx
      if: "!contains(runner.name, 'self-hosted')"
      uses: docker/setup-buildx-action@v3

    - name: 🔧 Set up sidecar BuildKit builder
      if: "contains(runner.name, 'self-hosted')"
      shell: bash
      run: |
        docker buildx create --name remote-builder --driver remote --use tcp://localhost:12345
        docker buildx inspect --bootstrap


    # Detect which cloud this runner is on (self-hosted ⇒ aws/azure, GH-hosted ⇒ unknown)
    - name: Detect cloud
      id: cloud
      uses: gitopsmanager/detect-cloud@main
      with:
        timeout-ms: 800


    # Set cache to ONE registry only (prefer the detected cloud)
    - name: set-cache-to-provider
      id: enhance
      uses: actions/github-script@v7
      env:
        ITEMS:    ${{ steps.normalize.outputs.items }}
        PROVIDER: ${{ steps.cloud.outputs.provider }}     # aws | azure | unknown
        PUSH:     ${{ inputs.push }}                      # none | aws | azure | both
        MODE:     ${{ inputs.buildkit_cache_mode }}       # none | min | max
        AWS_REG:  ${{ steps.env.outputs.aws_registry }}
        AZ_REG:   ${{ steps.env.outputs.azure_registry }}
      with:
        script: |
          const items = JSON.parse(process.env.ITEMS);
          const provider = (process.env.PROVIDER || 'unknown').toLowerCase();
          const PUSH = (process.env.PUSH || 'none').toLowerCase();
          const MODE = (process.env.MODE || 'none').toLowerCase();
          const AWS  = process.env.AWS_REG || '';
          const AZ   = process.env.AZ_REG  || '';

          // Choose exactly ONE cache registry
          let cacheCloud = '';
          if (provider === 'aws' || provider === 'azure') {
            cacheCloud = provider;                      // self-hosted, detected cloud
          } else {
            // GH-hosted or blocked IMDS: pick a sensible fallback based on what we're pushing
            if (PUSH === 'azure' || PUSH === 'both') cacheCloud = 'azure';
            else if (PUSH === 'aws')                cacheCloud = 'aws';
            else cacheCloud = ''; // not pushing anywhere; skip caching
          }

          const pick = (reg, img, mode) => reg ? {
            from: [`type=registry,ref=${reg}/${img}:buildcache`],
            to:   [`type=registry,ref=${reg}/${img}:buildcache,mode=${mode},ignore-error=true`]
          } : { from: [], to: [] };

          const out = items.map(it => {
            let cacheFrom = [], cacheTo = [];
            if (MODE !== 'none' && PUSH !== 'none') {
              if (cacheCloud === 'azure') ({ from: cacheFrom, to: cacheTo } = pick(AZ, it.image_name, MODE));
              if (cacheCloud === 'aws')   ({ from: cacheFrom, to: cacheTo } = pick(AWS, it.image_name, MODE));
            }
            return { ...it, cacheFrom, cacheTo };
          });

          core.setOutput('items', JSON.stringify(out));
          core.setOutput('count', String(out.length));

    # Generate bake file; (optionally) push to both registries in one build
    - name: generate-bake
      id: bakefile
      if: ${{ steps.enhance.outputs.count != '0' }}
      uses: actions/github-script@v7
      env:
        ITEMS:   ${{ steps.enhance.outputs.items }}
        PUSH:    ${{ inputs.push }}
        AWS_REG: ${{ steps.env.outputs.aws_registry }}
        AZ_REG:  ${{ steps.env.outputs.azure_registry }}
        TAG:     ${{ github.sha }}   # or another tag source
      with:
        script: |
          const fs = require('fs');
          const items = JSON.parse(process.env.ITEMS);
          const PUSH  = (process.env.PUSH || 'none').toLowerCase();
          const AWS   = process.env.AWS_REG || '';
          const AZ    = process.env.AZ_REG  || '';
          const TAG   = (process.env.TAG || '').slice(0,12) || 'dev';

          const target = {}, targets = [];
          items.forEach((it, i) => {
            const name = `t${i}`;
            const tags = [];

            // Build once, push to both registries if requested
            if (PUSH === 'aws' || PUSH === 'both')   tags.push(`${AWS}/${it.image_name}:${TAG}`);
            if (PUSH === 'azure' || PUSH === 'both') tags.push(`${AZ}/${it.image_name}:${TAG}`);
            if (PUSH === 'none' && it.image_name)    tags.push(`${it.image_name}:${TAG}`); // local tag only

            target[name] = {
              context: it.context,
              dockerfile: `${it.context}/${it.build_file}`,
              tags
            };
            if (it.cacheFrom?.length) target[name]['cache-from'] = it.cacheFrom;
            if (it.cacheTo?.length)   target[name]['cache-to']   = it.cacheTo;

            targets.push(name);
          });

          const bake = { group: { default: { targets } }, target };
          fs.writeFileSync('docker-bake.json', JSON.stringify(bake, null, 2));
          core.startGroup('docker-bake.json'); core.info(fs.readFileSync('docker-bake.json','utf8')); core.endGroup();

    # Bake with live workspace (includes files generated earlier)
    - name: bake
      uses: docker/bake-action@v6
      with:
        source: .
        files: docker-bake.json
        push: ${{ inputs.push != 'none' }}
        # builder: remote-builder

